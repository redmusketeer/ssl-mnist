{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "f2a606fc86d727d6901a90d3c9c69cc27007c7fd15fc5a22065c6c076d4dccc8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "# Used for Confusion Matrix\n",
    "from sklearn import metrics\n",
    "# Used for calculating result and accuracy\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Used for scaling image data\n",
    "from sklearn import preprocessing\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(input_shape):\n",
    "    num_classes = 10\n",
    "    (train_img, train_lbl), (test_img, test_lbl) = mnist.load_data()\n",
    "    input_dim = 28*28 \n",
    "    train_img = train_img.reshape(60000, input_dim) \n",
    "    test_img = test_img.reshape(10000, input_dim) \n",
    "    train_img = train_img.astype(\"float32\") / 255\n",
    "    test_img = test_img.astype(\"float32\") / 255\n",
    "    return train_img, train_lbl, test_img, test_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(y_test, y_test_pred, test_f1s, test_f1s_avg, pseudo_labels, epoch):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(25,25))\n",
    "    ax1.plot(range(epoch), test_f1s)\n",
    "    ax1.set_ylabel('f1 Score')\n",
    "    ax1.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], loc=\"lower right\")\n",
    "    ax2.plot(range(epoch), test_f1s_avg, )\n",
    "    ax2.set_ylabel('Average f1 Score')\n",
    "    ax2.legend(\"Average\", loc=\"lower right\")\n",
    "    ax3.bar(x=range(epoch), height=pseudo_labels)\n",
    "    ax3.set_ylabel('Pseudo-Labels Created')\n",
    "    ax3.set_xlabel('# Iterations')\n",
    " \n",
    "    # View confusion matrix after self-training\n",
    "    cf_matrix = confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    sns.heatmap(cf_matrix, annot=True, \n",
    "                fmt='.2f', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_dbn_model(input_shape, W, Bh):\r\n",
    "    dense_l0 = {}\r\n",
    "\r\n",
    "    if W is not None and Bh is not None:\r\n",
    "        dense_l0['weights'] = (W, Bh)\r\n",
    "\r\n",
    "    model = keras.models.Sequential(\r\n",
    "        [\r\n",
    "            keras.layers.Dense(500,input_shape=(784,), activation='relu', kernel_regularizer=keras.regularizers.L2(1e-5), kernel_initializer=keras.initializers.GlorotUniform(seed=1111), **dense_l0),\r\n",
    "            keras.layers.Dropout(0.5),\r\n",
    "            keras.layers.Dense(500, activation='relu', kernel_regularizer=keras.regularizers.L2(1e-5), kernel_initializer=keras.initializers.GlorotUniform(seed=1111)),\r\n",
    "            keras.layers.Dropout(0.5),\r\n",
    "            keras.layers.Dense(500, activation='relu', kernel_regularizer=keras.regularizers.L2(1e-5), kernel_initializer=keras.initializers.GlorotUniform(seed=1111)),\r\n",
    "            keras.layers.Dropout(0.5),\r\n",
    "            keras.layers.Dense(10, activation='softmax', kernel_initializer=keras.initializers.GlorotUniform(seed=2222))\r\n",
    "        ]\r\n",
    "    )\r\n",
    " \r\n",
    "    model.compile(\r\n",
    "        optimizer=keras.optimizers.Adam(0.001),\r\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\r\n",
    "    )\r\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "class Stopwatch(object):\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        if sys.platform == 'win32':\n",
    "            # on Windows, the best timer is time.clock()\n",
    "            self._timer_func = time.clock\n",
    "        else:\n",
    "            # on most other platforms, the best timer is time.time()\n",
    "            self._timer_func = time.time\n",
    "        self.reset()\n",
    "\n",
    "    def __enter__(self, verbose=False):\n",
    "        return self.start()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.stop()\n",
    "        return self.elapsed()\n",
    "\n",
    "    def start(self):\n",
    "        if not self._is_running:\n",
    "            self._start = self._timer_func()\n",
    "            self._is_running = True\n",
    "        return self\n",
    "\n",
    "    def stop(self):\n",
    "        if self._is_running:\n",
    "            self._total += (self._timer_func() - self._start)\n",
    "            self._is_running = False\n",
    "        return self\n",
    "\n",
    "    def elapsed(self):\n",
    "        if self._is_running:\n",
    "            now = self._timer_func()\n",
    "            self._total += (now - self._start)\n",
    "            self._start = now\n",
    "        if self.verbose:\n",
    "            print(\"Elapsed time: {0:.3f} sec\".format(self._total))\n",
    "        return self._total\n",
    "\n",
    "    def reset(self):\n",
    "        self._start = 0.\n",
    "        self._total = 0.\n",
    "        self._is_running = False\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, n_classes=None):\n",
    "    n_classes = n_classes or np.max(y) + 1\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_train_with_pretrain(num_pseudo: int, batchsize: int, epochs: int):\n",
    "    input_shape=(28, 28, 1)\n",
    "    X_train, y_train, X_test, y_test = get_data(input_shape=input_shape)\n",
    "    train_f1s = []\n",
    "    test_f1s = []\n",
    "    train_f1s_avg = []\n",
    "    test_f1s_avg = []\n",
    "    pseudo_labels = []\n",
    "    high_prob_counter = 1\n",
    "    total_pseudo_labelled = 0\n",
    "    total_prelabelled_added = 0\n",
    "    X_train_s = []\n",
    "    y_train_s = []\n",
    "    count = 0\n",
    "\n",
    "    (W, _, Bh) = rbm.get_weights()\n",
    "        with open('../RBM_weights/W.npy', 'wb') as f:\n",
    "            np.save(f,W)\n",
    "        with open('../RBM_weights/Bh.npy', 'wb') as f:\n",
    "            np.save(f,Bh)\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(60000):\n",
    "            if y_train[j]==i:\n",
    "                count+=1\n",
    "                y_train_s.append(y_train[j])\n",
    "                X_train_s.append(X_train[j])\n",
    "                if count > ((num_pseudo/10)-1):\n",
    "                    count = 0\n",
    "                    break\n",
    "    X_train_s = np.array(X_train_s)\n",
    "    y_train_s = np.array(y_train_s)\n",
    " \n",
    "    X_plabelled = np.empty((0,784))\n",
    "    y_plabelled = np.empty(0)\n",
    "    unlabelled_missed = np.empty((0,784))\n",
    " \n",
    "    i = 0\n",
    "    #pretrain 600 labelled\n",
    "    model_pretrained = define_dbn_model(input_shape, W, Bh)\n",
    "    with Stopwatch(verbose=True) as s:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience=12, verbose=2)\n",
    "            reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, verbose=2, patience=6, min_lr=1e-5)\n",
    "            callbacks = [early_stopping, reduce_lr]\n",
    "            try:\n",
    "                model_pretrained.fit(X_train_s, y_train_s,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False,\n",
    "                        callbacks=callbacks)\n",
    "            except KeyboardInterrupt:\n",
    "                pass\n",
    "\n",
    "    print(f\"{num_pseudo} pre-labelled data is training (pre-trained model is ready)...\")\n",
    " \n",
    "    X_train_s = np.empty((0,784))\n",
    "    y_train_s = np.empty(0)\n",
    " \n",
    "    total_prelabelled_added += num_pseudo\n",
    "    i += 1\n",
    "    epoch = 0\n",
    "    num_new_labeled = 0\n",
    " \n",
    "    while high_prob_counter > 0:\n",
    "        high_prob_counter = 0\n",
    "        epoch += 1\n",
    " \n",
    "        print(f\"Iteration: {epoch}\")\n",
    " \n",
    "        model = model_pretrained\n",
    " \n",
    "        if X_plabelled.shape[0] > num_pseudo:\n",
    " \n",
    "              X_train_s = np.append(X_train_s, X_plabelled[:num_pseudo], axis=0)\n",
    "              y_train_s = np.append(y_train_s, y_plabelled[:num_pseudo])\n",
    " \n",
    "              X_plabelled = X_plabelled[num_pseudo:]\n",
    "              y_plabelled = y_plabelled[num_pseudo:]\n",
    " \n",
    "              num_new_labeled += 1\n",
    "              print(f\"{num_pseudo*num_new_labeled} new labelled data is training...\")\n",
    "\n",
    "              with Stopwatch(verbose=True) as s:\n",
    "                  early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience=12, verbose=2)\n",
    "                  reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, verbose=2, patience=6, min_lr=1e-5)\n",
    "                  callbacks = [early_stopping, reduce_lr]\n",
    "                  try:\n",
    "                      model.fit(X_train_s, y_train_s,\n",
    "                              epochs=epochs,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False,\n",
    "                              callbacks=callbacks)\n",
    "                  except KeyboardInterrupt:\n",
    "                      pass\n",
    "              #model.fit(X_train_s, y_train_s, batch_size=batch_size, epochs=epochs, verbose=0)\n",
    " \n",
    "        y_test_pred = np.argmax(model.predict(X_test),axis=1)\n",
    "        y_train_pred = np.argmax(model.predict(X_train),axis=1)\n",
    " \n",
    "        train_f1 = f1_score(y_train, y_train_pred, average=None)\n",
    "        test_f1 = f1_score(y_test, y_test_pred, average=None)\n",
    " \n",
    "        print(f\"Train f1 Score: {train_f1}\")\n",
    "        print(f\"Test f1 Score: {test_f1}\")\n",
    " \n",
    "        train_f1s.append(train_f1)\n",
    "        test_f1s.append(test_f1)\n",
    " \n",
    "        \n",
    "        train_f1_avg = f1_score(y_train, y_train_pred, average='macro')\n",
    "        test_f1_avg = f1_score(y_test, y_test_pred, average='macro')\n",
    " \n",
    "        print(f\"Train f1 Score: {train_f1_avg}\")\n",
    "        print(f\"Test f1 Score: {test_f1_avg}\")\n",
    " \n",
    "        train_f1s_avg.append(train_f1_avg)\n",
    "        test_f1s_avg.append(test_f1_avg)\n",
    " \n",
    " \n",
    "        \n",
    "        num_unlabaled = num_pseudo * 1\n",
    "        if num_unlabaled*(i+1) < 60000:\n",
    "            unlabelled_splitted = X_train[(num_unlabaled*(i)):(num_unlabaled*(i+1))]\n",
    "            i += 1\n",
    "        elif unlabelled_missed.shape[0] > num_unlabaled:\n",
    "            unlabelled_splitted = unlabelled_missed[:num_unlabaled]\n",
    "            unlabelled_missed = unlabelled_missed[num_unlabaled:]\n",
    "        else:\n",
    "            unlabelled_splitted = unlabelled_missed\n",
    "            unlabelled_missed = np.empty((0,784))\n",
    " \n",
    "        if unlabelled_splitted.shape[0] == 0:\n",
    "            print(\"Whole dataset is observed. Nothing remained.\")\n",
    "            break\n",
    "        unlabeled_pred_prob = model.predict_proba(unlabelled_splitted)\n",
    " \n",
    "        print(f\"Now predicting labels for unlabeled data...\")\n",
    " \n",
    "        high_prob_idx = []\n",
    " \n",
    "        for j in range(unlabeled_pred_prob.shape[0]):\n",
    "            max = 0\n",
    "            pseudo_label = 0\n",
    "            pred_prob = unlabeled_pred_prob[j]\n",
    "            for k in range(pred_prob.shape[0]):\n",
    "                if pred_prob[k] > max:\n",
    "                    max = pred_prob[k]\n",
    "                    pseudo_label = k\n",
    "            if max > 0.95:\n",
    "                #print(j, max)\n",
    "                high_prob_counter += 1\n",
    "                high_prob_idx.append(j)\n",
    "                X_plabelled = np.append(X_plabelled, [unlabelled_splitted[j]], axis=0)\n",
    "                y_plabelled = np.append(y_plabelled, pseudo_label)\n",
    "            else:\n",
    "                unlabelled_missed = np.append(unlabelled_missed, [unlabelled_splitted[j]], axis=0)\n",
    " \n",
    "        print(f\"{high_prob_counter} high-probability unlabelled predictions is labelled and added to next train dataset.\")\n",
    " \n",
    "        print(f\"{unlabelled_missed.shape[0]} unlabelled instances remained to be predict in next iteration\")\n",
    " \n",
    "        print(f\"{X_plabelled.shape[0]} labelled instances are going to be added to train dataset in next iteration.\")\n",
    " \n",
    "        \n",
    "        total_pseudo_labelled += high_prob_counter\n",
    "        pseudo_labels.append(high_prob_counter)\n",
    "   \n",
    "    \n",
    "    print(\"*** Model's Learning is finished. ***\")\n",
    "    print(f\"In the end, {total_prelabelled_added} pre-labelled images and {total_pseudo_labelled} pseudo-labelled images added to train dataset\")\n",
    "    return y_test, y_test_pred, test_f1s, test_f1s_avg, pseudo_labels, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParams\n",
    "batch_size = 128\n",
    "epochs = 100 \n",
    "num_pseudo = 600\n",
    "y_test, y_test_pred, test_f1s, test_f1s_avg, pseudo_labels, epoch = self_train_with_pretrain(num_pseudo, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(y_test, y_test_pred, test_f1s, test_f1s_avg, pseudo_labels, epoch)"
   ]
  }
 ]
}