{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ssl_train_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XSqDIK1svI0"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "# Used for Confusion Matrix\n",
        "from sklearn import metrics\n",
        "# Used for Loading MNIST\n",
        "from struct import unpack\n",
        "# Used for calculating result and accuracy\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "# Used for scaling image data\n",
        "from sklearn import preprocessing\n",
        " \n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        " \n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        " \n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GemIbGss-ow"
      },
      "source": [
        "def get_data(input_shape):\n",
        "    num_classes = 10\n",
        "    (train_img, train_lbl), (test_img, test_lbl) = mnist.load_data()\n",
        "    #p=np.random.permutation(train_img.shape[0])\n",
        "    #train_img = train_img[p]\n",
        "    #train_lbl = train_lbl[p]\n",
        "    #p=np.random.permutation(test_img.shape[0])\n",
        "    #test_img = test_img[p]\n",
        "    #test_lbl = test_lbl[p]\n",
        "    train_img = train_img.astype(\"float32\") / 255\n",
        "    test_img = test_img.astype(\"float32\") / 255\n",
        "    # Make sure images have shape (28, 28, 1)\n",
        "    train_img = np.expand_dims(train_img, -1)\n",
        "    test_img = np.expand_dims(test_img, -1)\n",
        "    return train_img, train_lbl, test_img, test_lbl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4qbctOftGTJ"
      },
      "source": [
        "def define_cnn_model(input_shape):\n",
        "        model = keras.Sequential(\n",
        "            [\n",
        "                keras.Input(shape=input_shape),\n",
        "                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "                layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "                layers.Flatten(),\n",
        "                layers.Dropout(0.5),\n",
        "                layers.Dense(10, activation=\"softmax\"),\n",
        "            ]\n",
        "        )\n",
        " \n",
        "        model.compile(\n",
        "            optimizer=keras.optimizers.Adam(0.001),\n",
        "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
        "        )\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUrtiGiMtHAy"
      },
      "source": [
        "def plot_result(y_test, y_test_pred, test_f1s, test_f1s_avg, pseudo_labels, epoch):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(25,25))\n",
        "    ax1.plot(range(epoch), test_f1s)\n",
        "    ax1.set_ylabel('f1 Score')\n",
        "    ax1.set_xlabel('# Iterations')\n",
        "    ax1.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], loc=\"lower right\")\n",
        "    ax2.plot(range(epoch), test_f1s_avg, )\n",
        "    ax2.set_ylabel('Average f1 Score')\n",
        "    ax2.set_xlabel('# Iterations')\n",
        "    ax2.legend(\"Average\", loc=\"lower right\")\n",
        "    ax3.bar(x=range(epoch), height=pseudo_labels)\n",
        "    ax3.set_ylabel('Pseudo-Labels Created')\n",
        "    ax3.set_xlabel('# Iterations')\n",
        " \n",
        "    # View confusion matrix after self-training\n",
        "    cf_matrix = confusion_matrix(y_test, y_test_pred, normalize='true')\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    sns.heatmap(cf_matrix, annot=True, \n",
        "                fmt='.2f', cmap='Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I7gXJtEBKdZ"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "\n",
        "class Stopwatch(object):\n",
        "    \"\"\"\n",
        "    A simple cross-platform\n",
        "    context-manager stopwatch.\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import time\n",
        "    >>> with Stopwatch(verbose=True) as s:\n",
        "    ...     time.sleep(0.1) # doctest: +ELLIPSIS\n",
        "    Elapsed time: 0.10... sec\n",
        "    >>> with Stopwatch(verbose=False) as s:\n",
        "    ...     time.sleep(0.1)\n",
        "    >>> import math\n",
        "    >>> math.fabs(s.elapsed() - 0.1) < 0.05\n",
        "    True\n",
        "    \"\"\"\n",
        "    def __init__(self, verbose=False):\n",
        "        self.verbose = verbose\n",
        "        if sys.platform == 'win32':\n",
        "            # on Windows, the best timer is time.clock()\n",
        "            self._timer_func = time.clock\n",
        "        else:\n",
        "            # on most other platforms, the best timer is time.time()\n",
        "            self._timer_func = time.time\n",
        "        self.reset()\n",
        "\n",
        "    def __enter__(self, verbose=False):\n",
        "        return self.start()\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        self.stop()\n",
        "        return self.elapsed()\n",
        "\n",
        "    def start(self):\n",
        "        if not self._is_running:\n",
        "            self._start = self._timer_func()\n",
        "            self._is_running = True\n",
        "        return self\n",
        "\n",
        "    def stop(self):\n",
        "        if self._is_running:\n",
        "            self._total += (self._timer_func() - self._start)\n",
        "            self._is_running = False\n",
        "        return self\n",
        "\n",
        "    def elapsed(self):\n",
        "        if self._is_running:\n",
        "            now = self._timer_func()\n",
        "            self._total += (now - self._start)\n",
        "            self._start = now\n",
        "        if self.verbose:\n",
        "            print(\"Elapsed time: {0:.3f} sec\".format(self._total))\n",
        "        return self._total\n",
        "\n",
        "    def reset(self):\n",
        "        self._start = 0.\n",
        "        self._total = 0.\n",
        "        self._is_running = False\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enfq7Dhs-DaA"
      },
      "source": [
        "def self_train_with_pretrain(model_type: str, batchsize: int, epochs: int, num_pseudo: int):\n",
        "    input_shape=(28, 28, 1)\n",
        "    X_train, y_train, X_test, y_test = get_data(input_shape=input_shape)\n",
        "    train_f1s = []\n",
        "    test_f1s = []\n",
        "    train_f1s_avg = []\n",
        "    test_f1s_avg = []\n",
        "    pseudo_labels = []\n",
        "    high_prob_counter = 1\n",
        "    total_pseudo_labelled = 0\n",
        "    total_prelabelled_added = 0\n",
        "    \n",
        "    #X_train_s = np.empty((0,28,28,1))\n",
        "    #y_train_s = np.empty(0)\n",
        " \n",
        "    X_train_s = []\n",
        "    y_train_s = []\n",
        "    count = 0\n",
        "    for i in range(10):\n",
        "        for j in range(60000):\n",
        "            if y_train[j]==i:\n",
        "                count+=1\n",
        "                y_train_s.append(y_train[j])\n",
        "                X_train_s.append(X_train[j])\n",
        "                if count > ((num_pseudo/10)-1):\n",
        "                    count = 0\n",
        "                    break\n",
        "    X_train_s = np.array(X_train_s)\n",
        "    y_train_s = np.array(y_train_s)\n",
        " \n",
        "    X_plabelled = np.empty((0,28,28,1))\n",
        "    y_plabelled = np.empty(0)\n",
        "    unlabelled_missed = np.empty((0,28,28,1))\n",
        " \n",
        "    i = 0\n",
        "    #pretrain 600 labelled\n",
        "    if model_type == 'dense':\n",
        "        model_pretrained = define_dense_model(input_shape)\n",
        "    elif model_type =='CNN':\n",
        "        model_pretrained = define_cnn_model(input_shape)\n",
        " \n",
        "    with Stopwatch(verbose=True) as s:\n",
        "            early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience=30, verbose=2)\n",
        "            reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, verbose=2, patience=15, min_lr=1e-5)\n",
        "            callbacks = [early_stopping, reduce_lr]\n",
        "            try:\n",
        "                model_pretrained.fit(X_train_s, y_train_s,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        callbacks=callbacks)\n",
        "            except KeyboardInterrupt:\n",
        "                pass\n",
        "\n",
        "    print(f\"{num_pseudo} pre-labelled data is training (pre-trained model is ready)...\")\n",
        " \n",
        "    X_train_s = np.empty((0,28,28,1))\n",
        "    y_train_s = np.empty(0)\n",
        " \n",
        "    total_prelabelled_added += num_pseudo\n",
        "    i += 1\n",
        "    epoch = 0\n",
        "    num_new_labeled = 0\n",
        " \n",
        "    while high_prob_counter > 0:\n",
        "        high_prob_counter = 0\n",
        "\n",
        "        epoch += 1\n",
        " \n",
        "        print(f\"Iteration: {epoch}\")\n",
        " \n",
        "        model = model_pretrained\n",
        " \n",
        "        if X_plabelled.shape[0] > num_pseudo:\n",
        " \n",
        "              X_train_s = np.append(X_train_s, X_plabelled[:num_pseudo], axis=0)\n",
        "              y_train_s = np.append(y_train_s, y_plabelled[:num_pseudo])\n",
        " \n",
        "              X_plabelled = X_plabelled[num_pseudo:]\n",
        "              y_plabelled = y_plabelled[num_pseudo:]\n",
        " \n",
        "              num_new_labeled += 1\n",
        "              print(f\"{num_pseudo*num_new_labeled} new labelled data is training...\")\n",
        "\n",
        "              with Stopwatch(verbose=True) as s:\n",
        "                      early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience=30, verbose=2)\n",
        "                      reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, verbose=2, patience=15, min_lr=1e-5)\n",
        "                      callbacks = [early_stopping, reduce_lr]\n",
        "                      try:\n",
        "                          model.fit(X_train_s, y_train_s,\n",
        "                                  epochs=epochs,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=False,\n",
        "                                  callbacks=callbacks)\n",
        "                      except KeyboardInterrupt:\n",
        "                          pass\n",
        " \n",
        "        y_test_pred = np.argmax(model.predict(X_test),axis=1)\n",
        "        y_train_pred = np.argmax(model.predict(X_train),axis=1)\n",
        " \n",
        "        train_f1 = f1_score(y_train, y_train_pred, average=None)\n",
        "        test_f1 = f1_score(y_test, y_test_pred, average=None)\n",
        " \n",
        "        print(f\"Train f1 Score: {train_f1}\")\n",
        "        print(f\"Test f1 Score: {test_f1}\")\n",
        " \n",
        "        train_f1s.append(train_f1)\n",
        "        test_f1s.append(test_f1)\n",
        " \n",
        "        \n",
        "        train_f1_avg = f1_score(y_train, y_train_pred, average='macro')\n",
        "        test_f1_avg = f1_score(y_test, y_test_pred, average='macro')\n",
        " \n",
        "        print(f\"Train f1 Score: {train_f1_avg}\")\n",
        "        print(f\"Test f1 Score: {test_f1_avg}\")\n",
        " \n",
        "        train_f1s_avg.append(train_f1_avg)\n",
        "        test_f1s_avg.append(test_f1_avg)\n",
        "\n",
        "        num_unlabaled = num_pseudo * 1\n",
        "        if num_unlabaled*(i+1) < 60000:\n",
        "            unlabelled_splitted = X_train[(num_unlabaled*(i)):(num_unlabaled*(i+1))]\n",
        "            i += 1\n",
        "        elif unlabelled_missed.shape[0] > num_unlabaled:\n",
        "            unlabelled_splitted = unlabelled_missed[:num_unlabaled]\n",
        "            unlabelled_missed = unlabelled_missed[num_unlabaled:]\n",
        "        else:\n",
        "            unlabelled_splitted = unlabelled_missed\n",
        "            unlabelled_missed = np.empty((0,28,28,1))\n",
        "\n",
        "        if unlabelled_splitted.shape[0] == 0:\n",
        "            print(\"Whole dataset is observed. Nothing remained.\")\n",
        "            break\n",
        "        unlabeled_pred_prob = model.predict(unlabelled_splitted)\n",
        " \n",
        "        print(f\"Now predicting labels for unlabeled data...\")\n",
        " \n",
        "        high_prob_idx = []\n",
        " \n",
        "        for j in range(unlabeled_pred_prob.shape[0]):\n",
        "            max = 0\n",
        "            pseudo_label = 0\n",
        "            pred_prob = unlabeled_pred_prob[j]\n",
        "            for k in range(pred_prob.shape[0]):\n",
        "                if pred_prob[k] > max:\n",
        "                    max = pred_prob[k]\n",
        "                    pseudo_label = k\n",
        "            if max >= 0.95:\n",
        "                high_prob_counter += 1\n",
        "                high_prob_idx.append(j)\n",
        "                X_plabelled = np.append(X_plabelled, [unlabelled_splitted[j]], axis=0)\n",
        "                y_plabelled = np.append(y_plabelled, pseudo_label)\n",
        "            else:\n",
        "                unlabelled_missed = np.append(unlabelled_missed, [unlabelled_splitted[j]], axis=0)\n",
        " \n",
        "        print(f\"{high_prob_counter} high-probability unlabelled predictions is labelled and added to next train dataset.\")\n",
        " \n",
        "        print(f\"{unlabelled_missed.shape[0]} unlabelled instances remained to be predict in next iteration\")\n",
        " \n",
        "        print(f\"{X_plabelled.shape[0]} labelled instances are going to be added to train dataset in next iteration.\")\n",
        " \n",
        "        \n",
        "        total_pseudo_labelled += high_prob_counter\n",
        "        pseudo_labels.append(high_prob_counter)\n",
        "        \n",
        "    \n",
        "    print(\"*** Model's Learning is finished. ***\")\n",
        "    print(f\"In the end, {total_prelabelled_added} pre-labelled images and {total_pseudo_labelled} pseudo-labelled images added to train dataset\")\n",
        "    return y_test, y_test_pred, test_f1s, test_f1s_avg, pseudo_labels, epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hV7SGgsAXaY",
        "outputId": "43d9fc54-021a-4f1e-f271-0c649f0ce811"
      },
      "source": [
        "#HyperParams\n",
        "batch_size = 128\n",
        "epochs = 100 \n",
        "num_pseudo = 600\n",
        "y_test, y_test_pred, test_f1s, test_f1s_avg, pseudo_labels, epoch = self_train_with_pretrain('CNN', batch_size, epochs, num_pseudo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#call plot_result() here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfwDWxbKmVtR"
      },
      "source": [
        "#HyperParams\n",
        "batch_size = 64\n",
        "epochs = 100 \n",
        "num_pseudo = 100\n",
        "y_test, y_test_pred, test_f1s, test_f1s_avg, pseudo_labels, epoch = self_train_with_pretrain('CNN', batch_size, epochs, num_pseudo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#call plot_result() here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz5thBovmWsj"
      },
      "source": [
        "#HyperParams\n",
        "batch_size = 16\n",
        "epochs = 100 \n",
        "num_pseudo = 50\n",
        "y_test, y_test_pred, test_f1s, test_f1s_avg, pseudo_labels, epoch = self_train_with_pretrain('CNN', batch_size, epochs, num_pseudo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#call plot_result() here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJK_tqmu-HN1",
        "outputId": "3fea3c14-57e7-4d68-f32a-c0cbd396bd57"
      },
      "source": [
        "#HyperParams\n",
        "batch_size = 16\n",
        "epochs = 100 \n",
        "num_pseudo = 30\n",
        "y_test, y_test_pred, test_f1s, test_f1s_avg, pseudo_labels, epoch = self_train_with_pretrain('CNN', batch_size, epochs, num_pseudo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_bO8TR2W-MXt",
        "outputId": "494253fa-e02b-40eb-869e-fc5f4464890f"
      },
      "source": [
        "#call plot_result() here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}